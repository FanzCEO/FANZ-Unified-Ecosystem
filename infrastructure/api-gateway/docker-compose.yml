# üê≥ FANZ API Gateway - Docker Compose
#
# Complete Kong deployment with supporting services
# Includes Redis, PostgreSQL, Prometheus, and Kong Gateway

version: '3.8'

networks:
  fanz-network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16

volumes:
  kong-data:
    driver: local
  redis-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

services:
  # ===== DATABASE FOR KONG =====
  kong-database:
    image: postgres:15-alpine
    container_name: fanz-kong-db
    restart: unless-stopped
    environment:
      POSTGRES_DB: kong
      POSTGRES_USER: kong
      POSTGRES_PASSWORD: ${KONG_DB_PASSWORD:-k0ng_p4ssw0rd}
      POSTGRES_HOST_AUTH_METHOD: trust
    volumes:
      - kong-data:/var/lib/postgresql/data
    ports:
      - "5433:5432"  # Different port to avoid conflicts
    networks:
      fanz-network:
        ipv4_address: 172.20.1.10
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U kong -d kong"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # ===== KONG MIGRATIONS =====
  kong-migrations:
    image: kong:3.4.2-alpine
    container_name: fanz-kong-migrations
    depends_on:
      kong-database:
        condition: service_healthy
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_PORT: 5432
      KONG_PG_DATABASE: kong
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: ${KONG_DB_PASSWORD:-k0ng_p4ssw0rd}
    command: kong migrations bootstrap
    restart: "no"
    networks:
      - fanz-network

  # ===== REDIS FOR RATE LIMITING & CACHING =====
  redis:
    image: redis:7.2-alpine
    container_name: fanz-redis
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD:-r3d1s_p4ssw0rd} --appendonly yes
    volumes:
      - redis-data:/data
    ports:
      - "6380:6379"  # Different port to avoid conflicts
    networks:
      fanz-network:
        ipv4_address: 172.20.1.20
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "${REDIS_PASSWORD:-r3d1s_p4ssw0rd}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # ===== KONG GATEWAY =====
  kong:
    image: kong:3.4.2-alpine
    container_name: fanz-kong-gateway
    restart: unless-stopped
    depends_on:
      kong-database:
        condition: service_healthy
      redis:
        condition: service_healthy
      kong-migrations:
        condition: service_completed_successfully
    environment:
      # Database Configuration
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_PORT: 5432
      KONG_PG_DATABASE: kong
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: ${KONG_DB_PASSWORD:-k0ng_p4ssw0rd}
      
      # Gateway Configuration
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_ADMIN_GUI_LISTEN: 0.0.0.0:8002
      KONG_PROXY_LISTEN: 0.0.0.0:8000, 0.0.0.0:8443 ssl
      
      # SSL Configuration
      KONG_SSL_CERT: /etc/kong/ssl/fanz.crt
      KONG_SSL_CERT_KEY: /etc/kong/ssl/fanz.key
      
      # Plugin Configuration
      KONG_PLUGINS: bundled,oidc,prometheus,waf
      KONG_LUA_PACKAGE_PATH: /opt/?.lua;;
      
      # Redis Configuration
      KONG_REDIS_HOST: redis
      KONG_REDIS_PORT: 6379
      KONG_REDIS_PASSWORD: ${REDIS_PASSWORD:-r3d1s_p4ssw0rd}
      
      # Security
      KONG_ADMIN_GUI_AUTH: basic-auth
      KONG_ADMIN_GUI_SESSION_CONF: '{"secret":"${KONG_ADMIN_SESSION_SECRET:-fanz_admin_secret_key}","storage":"kong","cookie_secure":false}'
      
      # Logging
      KONG_LOG_LEVEL: info
      KONG_NGINX_HTTP_LOG_FORMAT: 'fanz_log $remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" $request_time $upstream_response_time "$http_x_forwarded_for" "$http_x_cluster" "$http_x_request_id"'
      
      # Performance
      KONG_NGINX_WORKER_PROCESSES: auto
      KONG_NGINX_WORKER_CONNECTIONS: 1024
      KONG_MEM_CACHE_SIZE: 128m
      
      # Custom Environment Variables
      OIDC_CLIENT_ID: ${OIDC_CLIENT_ID:-fanz-client-id}
      OIDC_CLIENT_SECRET: ${OIDC_CLIENT_SECRET:-fanz-client-secret}
      OIDC_DISCOVERY_URL: ${OIDC_DISCOVERY_URL:-https://auth.fanz.com/.well-known/openid-configuration}
      OIDC_REDIRECT_URI: ${OIDC_REDIRECT_URI:-https://api.fanz.com/auth/callback}
      OIDC_SESSION_SECRET: ${OIDC_SESSION_SECRET:-oidc_session_secret_key}
      OIDC_INTROSPECTION_ENDPOINT: ${OIDC_INTROSPECTION_ENDPOINT:-https://auth.fanz.com/oauth/introspect}
      
      # API Keys
      FANZ_ADMIN_API_KEY: ${FANZ_ADMIN_API_KEY:-admin-key-12345}
      FANZ_FINANCE_API_KEY: ${FANZ_FINANCE_API_KEY:-finance-key-12345}
      FANZ_MEDIA_API_KEY: ${FANZ_MEDIA_API_KEY:-media-key-12345}
      
      # JWT Secrets
      JWT_SECRET_ADMIN: ${JWT_SECRET_ADMIN:-admin-jwt-secret-key}
      JWT_SECRET_FINANCE: ${JWT_SECRET_FINANCE:-finance-jwt-secret-key}
      
    volumes:
      - ./kong-config.yaml:/etc/kong/kong.yaml:ro
      - ./ssl:/etc/kong/ssl:ro
      - ./plugins:/opt/kong/plugins:ro
      - ./logs:/var/log/kong
      - ./blocklist.txt:/etc/kong/blocklist.txt:ro
    ports:
      # Proxy (HTTP/HTTPS)
      - "8000:8000"   # HTTP Proxy
      - "8443:8443"   # HTTPS Proxy
      # Admin API
      - "8001:8001"   # Admin API
      - "8002:8002"   # Admin GUI
      # Monitoring
      - "9542:9542"   # Prometheus metrics
    networks:
      fanz-network:
        ipv4_address: 172.20.1.30
    healthcheck:
      test: ["CMD", "kong", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ===== KONG DECLARATIVE CONFIG LOADER =====
  kong-config-loader:
    image: kong:3.4.2-alpine
    container_name: fanz-kong-config
    depends_on:
      kong:
        condition: service_healthy
    environment:
      KONG_ADMIN_URL: http://kong:8001
    volumes:
      - ./kong-config.yaml:/kong-config.yaml:ro
      - ./scripts/load-config.sh:/load-config.sh:ro
    command: ["/load-config.sh"]
    restart: "no"
    networks:
      - fanz-network

  # ===== PROMETHEUS FOR METRICS =====
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: fanz-prometheus
    restart: unless-stopped
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    ports:
      - "9090:9090"
    networks:
      fanz-network:
        ipv4_address: 172.20.1.40
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===== GRAFANA FOR DASHBOARDS =====
  grafana:
    image: grafana/grafana:10.0.0
    container_name: fanz-grafana
    restart: unless-stopped
    depends_on:
      - prometheus
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-fanz_admin}
      GF_INSTALL_PLUGINS: redis-datasource,grafana-piechart-panel
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY:-grafana_secret_key}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_ANALYTICS_CHECK_FOR_UPDATES: false
      GF_SERVER_ROOT_URL: https://monitoring.fanz.com/
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    ports:
      - "3001:3000"  # Different port to avoid conflicts
    networks:
      fanz-network:
        ipv4_address: 172.20.1.50
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===== ELASTICSEARCH FOR LOGS =====
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: fanz-elasticsearch
    restart: unless-stopped
    environment:
      discovery.type: single-node
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
      xpack.security.enabled: false
      xpack.security.enrollment.enabled: false
      xpack.security.http.ssl.enabled: false
      xpack.security.transport.ssl.enabled: false
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      fanz-network:
        ipv4_address: 172.20.1.60
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===== LOGSTASH FOR LOG PROCESSING =====
  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    container_name: fanz-logstash
    restart: unless-stopped
    depends_on:
      elasticsearch:
        condition: service_healthy
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
    ports:
      - "5044:5044"  # Beats input
      - "9600:9600"  # Logstash monitoring
    networks:
      fanz-network:
        ipv4_address: 172.20.1.70
    environment:
      LS_JAVA_OPTS: "-Xmx512m -Xms512m"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9600 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===== KIBANA FOR LOG VISUALIZATION =====
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: fanz-kibana
    restart: unless-stopped
    depends_on:
      elasticsearch:
        condition: service_healthy
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
      SERVER_NAME: kibana.fanz.com
      SERVER_HOST: 0.0.0.0
      XPACK_SECURITY_ENABLED: false
      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY: ${KIBANA_ENCRYPTION_KEY:-kibana_encryption_key_32_chars}
    ports:
      - "5601:5601"
    networks:
      fanz-network:
        ipv4_address: 172.20.1.80
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===== JAEGER FOR DISTRIBUTED TRACING =====
  jaeger:
    image: jaegertracing/all-in-one:1.46
    container_name: fanz-jaeger
    restart: unless-stopped
    environment:
      COLLECTOR_ZIPKIN_HOST_PORT: ":9411"
      COLLECTOR_OTLP_ENABLED: true
    ports:
      - "16686:16686"  # Jaeger UI
      - "14268:14268"  # Jaeger collector
      - "14250:14250"  # Jaeger gRPC
      - "9411:9411"    # Zipkin collector
    networks:
      fanz-network:
        ipv4_address: 172.20.1.90
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:16686/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  elasticsearch-data:
    driver: local

# ===== DEVELOPMENT OVERRIDES =====
  # To use development settings, create docker-compose.override.yml with:
  # version: '3.8'
  # services:
  #   kong:
  #     environment:
  #       KONG_LOG_LEVEL: debug
  #       KONG_ADMIN_GUI_AUTH: off
  #     ports:
  #       - "8000:8000"
  #       - "8443:8443"
  #       - "8001:8001"
  #       - "8002:8002"